{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jawiki-GPT-2-demo.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/layolu/gpt-2/blob/fromscratch/jawiki_GPT_2_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cml7EpFW9TZl",
        "colab_type": "text"
      },
      "source": [
        "# GPT-2 日本語版Wikipedia 学習済みモデル (117M) デモNotebook\n",
        "作成者: layolu https://twitter.com/layolu\n",
        "\n",
        "\n",
        "一時期「危険すぎる」とのことで公開が見送られていたなど巷で話題の OpenAI の文書生成モデル \"GPT-2\" \n",
        "([公式記事](https://openai.com/blog/better-language-models/), [参考記事1](https://japan.zdnet.com/article/35145106/), [参考記事2](https://gigazine.net/news/20191106-gpt-2-final-model-release/)\n",
        "について、日本語での学習済みモデルが見当たらなかったため、2019/11の日本語版Wikipediaの全記事データで学習させたものを公開します。モデルサイズは117M (OpenAIが最初に公開したいわゆるsmallモデルと同様) です。\n",
        "\n",
        "またこのColabノートブックでは、数回クリックすることでかんたんに、しかも無料でこのモデルを試すことができます。\n",
        "\n",
        "## 学習済みモデルのダウンロード\n",
        "[Google Drive](https://drive.google.com/file/d/1NpZcg_cVZ-WkjGOH14LFzg70196KRA6f/view?usp=sharing)で公開しています (1.7GB)。\n",
        "このノートブックで試す限りはダウンロードは不要です。\n",
        "\n",
        "## デモの遊び方\n",
        "1. 左上の [PLAYGROUNDで開く] をクリックします。Googleへログインを求められた場合ログインします。\n",
        "1. メニューの [ランタイム] → [すべてのセルを実行] をクリックすると、警告ダイアログが表示されるため [そのまま実行]をクリックします。自動でモデルのダウンロードはじめ環境のセットアップが行われます (数分かかります)。<br>\n",
        "セットアップが完了すると、一番下の \"入力フォーム\" のセルにフォームが表示されます。\n",
        "1. テキストエリアに文章を入力し \"続きを生成\" ボタンを押すと、GPT-2がその続きを自動生成し、結果が下に表示されます (CPU環境で50秒ほどかかります)。<br>\n",
        "\n",
        "## TIPS\n",
        "* 学習データ内で記事間の区切りに `<|endoftext|>` という文字列を使う仕様のため、入力として `<|endoftext|>` を与えるとランダムなWikipediaのフェイク記事が作成されます。\n",
        "* また、`<|endoftext|>` の後に続けて文章を書くと、そこからWikipedia風の文章が作成されます (内容については全く保証できませんが）。<br>\n",
        "おすすめとして、 `<|endoftext|>『` を与えると色々なジャンルの架空の作品やテレビ番組等の記事が頭から自動作成されます (内容については(略))。\n",
        "\n",
        "## 利用したコード\n",
        "* https://github.com/nshepperd/gpt-2/\n",
        "OpenAIのオリジナルコードをもとに学習可能にしたフォーク\n",
        "* https://github.com/rkfg/gpt-2/ 上記 nshepperd さんフォークから、さらに多国語対応のため[Sentencepiece](https://github.com/google/sentencepiece)に対応したフォーク\n",
        "\n",
        "学習は rkfg さんのフォークで行いましたが、nshepperd さんフォークにはより自然な出力ができる top_p という実装が入っているため ([参考](https://www.gwern.net/GPT-2#training-gpt-2-poetry))、このデモではそのコードを rkfg さんフォークに組み込んだものを使っています ( https://github.com/layolu/gpt-2/ )。\n",
        "\n",
        "Sentencepieceを利用しているため、このモデルは rkfg さんのフォーク（及び私のフォーク）でのみ動作します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frSSaRZvnGhl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title モデルのダウンロードとソフトウェアのインストール\n",
        "%%bash\n",
        "[ ! -f ready ] && \\\n",
        "    curl gdrive.sh | bash -s \"https://drive.google.com/file/d/1NpZcg_cVZ-WkjGOH14LFzg70196KRA6f/view?usp=sharing\" && \\\n",
        "    unzip jawiki-117M-292612.zip && \\\n",
        "    mkdir models && \\\n",
        "    mv jawiki-117M-292612 models && \\\n",
        "    git clone https://github.com/layolu/gpt-2 && \\\n",
        "    pip install sentencepiece fire && \\\n",
        "    touch ready"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1l6HFYzTC00",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Pythonパッケージのインポート\n",
        "%tensorflow_version 1.x \n",
        "import IPython\n",
        "from google.colab import output as colab_output\n",
        "import fire\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "sys.path.append('gpt-2/src')\n",
        "import model, sample, encoder_sp as encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS_1cjezrFtu",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title 学習済みモデルのローディング\n",
        "#from https://github.com/nshepperd/gpt-2/blob/finetuning/src/interactive_conditional_samples.py\n",
        "# and https://github.com/rkfg/gpt-2/blob/fromscratch/src/interactive_conditional_samples.py\n",
        "model_name = 'jawiki-117M-292612'\n",
        "seed = None\n",
        "nsamples = 1\n",
        "batch_size = 1\n",
        "length = None\n",
        "temperature =1\n",
        "top_k = 0\n",
        "top_p = 0.9\n",
        "\n",
        "assert nsamples % batch_size == 0\n",
        "\n",
        "enc = encoder.get_encoder(model_name)\n",
        "hparams = model.default_hparams()\n",
        "with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
        "    hparams.override_from_dict(json.load(f))\n",
        "\n",
        "if length is None:\n",
        "    length = hparams.n_ctx // 2\n",
        "elif length > hparams.n_ctx:\n",
        "    raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "output = sample.sample_sequence(\n",
        "    hparams=hparams, length=length,\n",
        "    context=context,\n",
        "    batch_size=batch_size,\n",
        "    temperature=temperature, top_k=top_k, top_p=top_p\n",
        ")\n",
        "saver = tf.train.Saver()\n",
        "ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name, 'checkpoint/run1'))\n",
        "saver.restore(sess, ckpt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C89FlmYxtyKT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title テキスト生成用関数\n",
        "def generate_text(input_text):\n",
        "    context_tokens = enc.encode(input_text.replace('\\n', '<|n|>'))\n",
        "    generated = 0\n",
        "    for _ in range(nsamples // batch_size):\n",
        "        out = sess.run(output, feed_dict={\n",
        "            context: [context_tokens for _ in range(batch_size)]\n",
        "        })[:, len(context_tokens):]\n",
        "        for i in range(batch_size):\n",
        "            generated += 1\n",
        "            gen_text = enc.decode(out[i])\n",
        "        html = f'<strong><u>{input_text}</u></strong>{gen_text}'\n",
        "        html = html.replace('\\n', '<br>')\n",
        "        html = html.replace('<|endoftext|>', '<hr>')\n",
        "    return IPython.display.HTML(f'<p>{html}</p>')\n",
        "colab_output.register_callback('notebook.generate', generate_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvNeTPPCUFNL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title 入力フォーム\n",
        "%%html\n",
        "<div name=\"gpt-2-interface\">\n",
        "    <textarea id=\"gpt-2-context\" rows=\"6\" cols=\"160\">おいしいカレーの作り方は、</textarea>\n",
        "    <br>\n",
        "    <button id=\"gpt-2-generate\" onclick=\"generate()\">続きを生成</button>\n",
        "    <div id=\"gpt-2-result\"></div>\n",
        "</div>\n",
        "<script>\n",
        "async function generate() {\n",
        "    console.time('generate')\n",
        "    document.querySelector(\"#gpt-2-generate\").disabled = true;\n",
        "    const context = document.querySelector(\"#gpt-2-context\").value;\n",
        "    console.log(context)\n",
        "    const result = await google.colab.kernel.invokeFunction(\n",
        "    'notebook.generate', // The callback name.\n",
        "    [context], // The arguments.\n",
        "    {}); // kwargs\n",
        "    console.dir(result);\n",
        "    const html = result.data['text/html'];\n",
        "    document.querySelector(\"#gpt-2-result\").innerHTML = html;\n",
        "    document.querySelector(\"#gpt-2-generate\").disabled = false;\n",
        "    console.timeEnd('generate')\n",
        "};\n",
        "</script>"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}